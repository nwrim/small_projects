{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://knoesis.org/resources/images/knoesis_depression_logo.jpg\" alt=\"Knoesis Depression Project Logo\" style=\"float:right;width: 250px;\"/>\n",
    "\n",
    "# Social-media Depression Detector (SDD)\n",
    "\n",
    "#### This notebook executes the code developed to detect depression using the ssToT method introduced in our ASONAM 2017 paper titled \"Semi-Supervised Approach to Monitoring Clinical Depressive Symptoms in Social Media\"\n",
    "\n",
    "This software is open-source, released under the terms of GPL-3.0 and CreativesForGood licenses.\n",
    "\n",
    "##### Author: Hussein S. Al-Olimat (github.com/halolimat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, json, string, datetime, random, itertools\n",
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "# You should install the following libraries\n",
    "import wordsegment #https://pypi.python.org/pypi/wordsegment\n",
    "from nltk import TweetTokenizer #http://www.nltk.org/api/nltk.tokenize.html\n",
    "from textblob import TextBlob #https://textblob.readthedocs.io/en/dev/\n",
    "from gensim import corpora #https://radimrehurek.com/gensim/\n",
    "import pandas as pd #http://pandas.pydata.org/\n",
    "import numpy as NP #http://www.numpy.org/\n",
    "import matplotlib.pyplot as plt #https://matplotlib.org/\n",
    "\n",
    "# You should install pSSLDA in order to be able to run this program and import these libraries\n",
    "#     follow the instruction in: https://github.com/davidandrzej/pSSLDA\n",
    "import FastLDA\n",
    "from pSSLDA import infer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### Preparing the depression lexicon to seed the LDA topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read Depression PHQ-9 Lexicon (DPL) from json file\n",
    "with open(\"depression_lexicon.json\") as f:\n",
    "    seed_terms = json.load(f)\n",
    "\n",
    "# read all seed terms into a list removing the underscore from all seeds\n",
    "all_seeds_raw = [seed.replace(\"_\",\" \").encode('utf-8') for seed in list(itertools.chain.from_iterable([seed_terms[signal] for signal in seed_terms.keys()]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### Preparing other lexicons and resources to be used in filtering and preprocessing the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Other lexicons and resources\n",
    "emojies = [\":‑)\", \":)\", \":D\", \":o)\", \":]\", \":3\", \":c)\", \":>\", \"=]\", \"8)\", \"=)\", \":}\", \":^)\", \":っ)\", \":‑D\", \"8‑D\", \"8D\", \"x‑D\", \"xD\", \"X‑D\", \"XD\", \"=‑D\", \"=D\", \"=‑3\", \"=3\", \"B^D\", \":-))\", \">:[\", \":‑(\", \":(\", \":‑c\", \":c\", \":‑<\", \":っC\", \":<\", \":‑[\", \":[\", \":{\", \";(\", \":-||\", \":@\", \">:(\", \":'‑(\", \":'(\", \":'‑)\", \":')\", \"D:<\", \"D:\", \"D8\", \"D;\", \"D=\", \"DX\", \"v.v\", \"D‑':\", \">:O\", \":‑O\", \":O\", \":‑o\", \":o\", \"8‑0\", \"O_O\", \"o‑o\", \"O_o\", \"o_O\", \"o_o\", \"O-O\", \":*\", \":-*\", \":^*\", \"(\", \"}{'\", \")\", \";‑)\", \";)\", \"*-)\", \"*)\", \";‑]\", \";]\", \";D\", \";^)\", \":‑,\", \">:P\", \":‑P\", \":P\", \"X‑P\", \"x‑p\", \"xp\", \"XP\", \":‑p\", \":p\", \"=p\", \":‑Þ\", \":Þ\", \":þ\", \":‑þ\", \":‑b\", \":b\", \"d:\", \">:\\\\\", \">:/\", \":‑/\", \":‑.\", \":/\", \":\\\\\", \"=/\", \"=\\\\\", \":L\", \"=L\", \":S\", \">.<\", \":|\", \":‑|\", \":$\", \":‑X\", \":X\", \":‑#\", \":#\", \"O:‑)\", \"0:‑3\", \"0:3\", \"0:‑)\", \"0:)\", \"0;^)\", \">:)\", \">;)\", \">:‑)\", \"}:‑)\", \"}:)\", \"3:‑)\", \"3:)\", \"o/\\o\", \"^5\", \">_>^\", \"^<_<\", \"|;‑)\", \"|‑O\", \":‑J\", \":‑&\", \":&\", \"#‑)\", \"%‑)\", \"%)\", \":‑###..\", \":###..\", \"<:‑|\", \"<*)))‑{\", \"><(((*>\", \"><>\", \"\\o/\", \"*\\0/*\", \"@}‑;‑'‑‑‑\", \"@>‑‑>‑‑\", \"~(_8^(I)\", \"5:‑)\", \"~:‑\\\\\", \"//0‑0\\\\\\\\\", \"*<|:‑)\", \"=:o]\", \"7:^]\", \",:‑)\", \"</3\", \"<3\"]\n",
    "\n",
    "# Tweet tokenizer from NLTK: http://www.nltk.org/_modules/nltk/tokenize/casual.html#TweetTokenizer\n",
    "nltk_tok = TweetTokenizer(preserve_case=True, reduce_len=True, strip_handles=True)\n",
    "\n",
    "printable = set(string.printable)\n",
    "\n",
    "punctuation = list(string.punctuation)\n",
    "punctuation.remove(\"-\")\n",
    "punctuation.remove('_')\n",
    "\n",
    "long_stop_list = [\"a\", \"a's\", \"abaft\", \"able\", \"aboard\", \"about\", \"above\", \"abst\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\", \"added\", \"adj\", \"affected\", \"affecting\", \"affects\", \"afore\", \"aforesaid\", \"after\", \"afterwards\", \"again\", \"against\", \"agin\", \"ago\", \"ah\", \"ain't\", \"aint\", \"albeit\", \"all\", \"allow\", \"allows\", \"almost\", \"alone\", \"along\", \"alongside\", \"already\", \"also\", \"although\", \"always\", \"am\", \"american\", \"amid\", \"amidst\", \"among\", \"amongst\", \"an\", \"and\", \"anent\", \"announce\", \"another\", \"any\", \"anybody\", \"anyhow\", \"anymore\", \"anyone\", \"anything\", \"anyway\", \"anyways\", \"anywhere\", \"apart\", \"apparently\", \"appear\", \"appreciate\", \"appropriate\", \"approximately\", \"are\", \"aren\", \"aren't\", \"arent\", \"arise\", \"around\", \"as\", \"aside\", \"ask\", \"asking\", \"aslant\", \"associated\", \"astride\", \"at\", \"athwart\", \"auth\", \"available\", \"away\", \"awfully\", \"b\", \"back\", \"bar\", \"barring\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"begin\", \"beginning\", \"beginnings\", \"begins\", \"behind\", \"being\", \"believe\", \"below\", \"beneath\", \"beside\", \"besides\", \"best\", \"better\", \"between\", \"betwixt\", \"beyond\", \"biol\", \"both\", \"brief\", \"briefly\", \"but\", \"by\", \"c\", \"c'mon\", \"c's\", \"ca\", \"came\", \"can\", \"can't\", \"cannot\", \"cant\", \"cause\", \"causes\", \"certain\", \"certainly\", \"changes\", \"circa\", \"clearly\", \"close\", \"co\", \"com\", \"come\", \"comes\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"contain\", \"containing\", \"contains\", \"corresponding\", \"cos\", \"could\", \"couldn\", \"couldn't\", \"couldnt\", \"couldst\", \"course\", \"currently\", \"d\", \"dare\", \"dared\", \"daren\", \"dares\", \"daring\", \"date\", \"definitely\", \"described\", \"despite\", \"did\", \"didn\", \"didn't\", \"different\", \"directly\", \"do\", \"does\", \"doesn\", \"doesn't\", \"doing\", \"don\", \"don't\", \"done\", \"dost\", \"doth\", \"down\", \"downwards\", \"due\", \"during\", \"durst\", \"e\", \"each\", \"early\", \"ed\", \"edu\", \"effect\", \"eg\", \"eight\", \"eighty\", \"either\", \"else\", \"elsewhere\", \"em\", \"end\", \"ending\", \"english\", \"enough\", \"entirely\", \"er\", \"ere\", \"especially\", \"et\", \"et-al\", \"etc\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"exactly\", \"example\", \"except\", \"excepting\", \"f\", \"failing\", \"far\", \"few\", \"ff\", \"fifth\", \"first\", \"five\", \"fix\", \"followed\", \"following\", \"follows\", \"for\", \"former\", \"formerly\", \"forth\", \"found\", \"four\", \"from\", \"further\", \"furthermore\", \"g\", \"gave\", \"get\", \"gets\", \"getting\", \"give\", \"given\", \"gives\", \"giving\", \"go\", \"goes\", \"going\", \"gone\", \"gonna\", \"got\", \"gotta\", \"gotten\", \"greetings\", \"h\", \"had\", \"hadn\", \"hadn't\", \"happens\", \"hard\", \"hardly\", \"has\", \"hasn\", \"hasn't\", \"hast\", \"hath\", \"have\", \"haven\", \"haven't\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"hed\", \"hello\", \"help\", \"hence\", \"her\", \"here\", \"here's\", \"hereafter\", \"hereby\", \"herein\", \"heres\", \"hereupon\", \"hers\", \"herself\", \"hes\", \"hi\", \"hid\", \"high\", \"him\", \"himself\", \"his\", \"hither\", \"home\", \"hopefully\", \"how\", \"how's\", \"howbeit\", \"however\", \"hundred\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"id\", \"ie\", \"if\", \"ignored\", \"ill\", \"im\", \"immediate\", \"immediately\", \"importance\", \"important\", \"in\", \"inasmuch\", \"inc\", \"indeed\", \"index\", \"indicate\", \"indicated\", \"indicates\", \"information\", \"inner\", \"inside\", \"insofar\", \"instantly\", \"instead\", \"into\", \"invention\", \"inward\", \"is\", \"isn\", \"isn't\", \"it\", \"it'd\", \"it'll\", \"it's\", \"itd\", \"its\", \"itself\", \"j\", \"just\", \"k\", \"keep\", \"keeps\", \"kept\", \"kg\", \"km\", \"know\", \"known\", \"knows\", \"l\", \"large\", \"largely\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"least\", \"left\", \"less\", \"lest\", \"let\", \"let's\", \"lets\", \"like\", \"liked\", \"likely\", \"likewise\", \"line\", \"little\", \"living\", \"ll\", \"long\", \"look\", \"looking\", \"looks\", \"ltd\", \"m\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\", \"maybe\", \"mayn\", \"me\", \"mean\", \"means\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"mid\", \"midst\", \"might\", \"mightn\", \"million\", \"mine\", \"minus\", \"miss\", \"ml\", \"more\", \"moreover\", \"most\", \"mostly\", \"mr\", \"mrs\", \"much\", \"mug\", \"must\", \"mustn\", \"mustn't\", \"my\", \"myself\", \"n\", \"na\", \"name\", \"namely\", \"nay\", \"nd\", \"near\", \"nearly\", \"neath\", \"necessarily\", \"necessary\", \"need\", \"needed\", \"needing\", \"needn\", \"needs\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\", \"nigh\", \"nigher\", \"nighest\", \"nine\", \"ninety\", \"nisi\", \"no\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"nor\", \"normally\", \"nos\", \"not\", \"noted\", \"nothing\", \"notwithstanding\", \"novel\", \"now\", \"nowhere\", \"o\", \"obtain\", \"obtained\", \"obviously\", \"of\", \"off\", \"often\", \"oh\", \"ok\", \"okay\", \"old\", \"omitted\", \"on\", \"once\", \"one\", \"ones\", \"oneself\", \"only\", \"onto\", \"open\", \"or\", \"ord\", \"other\", \"others\", \"otherwise\", \"ought\", \"oughtn\", \"our\", \"ours\", \"ourselves\", \"out\", \"outside\", \"over\", \"overall\", \"owing\", \"own\", \"p\", \"page\", \"pages\", \"part\", \"particular\", \"particularly\", \"past\", \"pending\", \"per\", \"perhaps\", \"placed\", \"please\", \"plus\", \"poorly\", \"possible\", \"possibly\", \"potentially\", \"pp\", \"predominantly\", \"present\", \"presumably\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provided\", \"provides\", \"providing\", \"public\", \"put\", \"q\", \"qua\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \"ran\", \"rather\", \"rd\", \"re\", \"readily\", \"real\", \"really\", \"reasonably\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \"regardless\", \"regards\", \"related\", \"relatively\", \"research\", \"respecting\", \"respectively\", \"resulted\", \"resulting\", \"results\", \"right\", \"round\", \"run\", \"s\", \"said\", \"same\", \"sans\", \"save\", \"saving\", \"saw\", \"say\", \"saying\", \"says\", \"sec\", \"second\", \"secondly\", \"section\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sensible\", \"sent\", \"serious\", \"seriously\", \"seven\", \"several\", \"shall\", \"shalt\", \"shan\", \"shan't\", \"she\", \"she'd\", \"she'll\", \"she's\", \"shed\", \"shell\", \"shes\", \"short\", \"should\", \"shouldn\", \"shouldn't\", \"show\", \"showed\", \"shown\", \"showns\", \"shows\", \"significant\", \"significantly\", \"similar\", \"similarly\", \"since\", \"six\", \"slightly\", \"small\", \"so\", \"some\", \"somebody\", \"somehow\", \"someone\", \"somethan\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"special\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"still\", \"stop\", \"strongly\", \"sub\", \"substantially\", \"successfully\", \"such\", \"sufficiently\", \"suggest\", \"summat\", \"sup\", \"supposing\", \"sure\", \"t\", \"t's\", \"take\", \"taken\", \"taking\", \"tell\", \"tends\", \"th\", \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"that'll\", \"that's\", \"that've\", \"thats\", \"the\", \"thee\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"there'll\", \"there's\", \"there've\", \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\", \"thereof\", \"therere\", \"theres\", \"thereto\", \"thereupon\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"theyd\", \"theyre\", \"thine\", \"think\", \"third\", \"this\", \"tho\", \"thorough\", \"thoroughly\", \"those\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"three\", \"thro\", \"throug\", \"through\", \"throughout\", \"thru\", \"thus\", \"thyself\", \"til\", \"till\", \"tip\", \"to\", \"today\", \"together\", \"too\", \"took\", \"touching\", \"toward\", \"towards\", \"tried\", \"tries\", \"true\", \"truly\", \"try\", \"trying\", \"ts\", \"twas\", \"tween\", \"twere\", \"twice\", \"twill\", \"twixt\", \"two\", \"twould\", \"u\", \"un\", \"under\", \"underneath\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"until\", \"unto\", \"up\", \"upon\", \"ups\", \"us\", \"use\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"uses\", \"using\", \"usually\", \"v\", \"value\", \"various\", \"ve\", \"versus\", \"very\", \"via\", \"vice\", \"vis-a-vis\", \"viz\", \"vol\", \"vols\", \"vs\", \"w\", \"wanna\", \"want\", \"wanting\", \"wants\", \"was\", \"wasn\", \"wasn't\", \"wasnt\", \"way\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"wed\", \"welcome\", \"well\", \"went\", \"were\", \"weren\", \"weren't\", \"werent\", \"wert\", \"what\", \"what'll\", \"what's\", \"whatever\", \"whats\", \"when\", \"when's\", \"whence\", \"whencesoever\", \"whenever\", \"where\", \"where's\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"whichever\", \"whichsoever\", \"while\", \"whilst\", \"whim\", \"whither\", \"who\", \"who'll\", \"who's\", \"whod\", \"whoever\", \"whole\", \"whom\", \"whomever\", \"whore\", \"whos\", \"whose\", \"whoso\", \"whosoever\", \"why\", \"why's\", \"widely\", \"will\", \"willing\", \"wish\", \"with\", \"within\", \"without\", \"won't\", \"wonder\", \"wont\", \"words\", \"world\", \"would\", \"wouldn\", \"wouldn't\", \"wouldnt\", \"wouldst\", \"www\", \"x\", \"y\", \"ye\", \"yes\", \"yet\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"youd\", \"your\", \"youre\", \"yours\", \"yourself\", \"yourselves\", \"z\", \"zero\"]\n",
    "stoplist = long_stop_list + punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##### Loading GeoText data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>USER_79321756</td>\n",
       "      <td>2010-03-03T04:15:26</td>\n",
       "      <td>RT @USER_2ff4faca: IF SHE DO IT 1 MORE TIME......</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>USER_79321756</td>\n",
       "      <td>2010-03-03T04:55:32</td>\n",
       "      <td>@USER_77a4822d @USER_2ff4faca okay:) lol. Sayi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>USER_79321756</td>\n",
       "      <td>2010-03-03T05:13:34</td>\n",
       "      <td>RT @USER_5d4d777a: YOURE A FAG FOR GETTING IN ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USER_79321756</td>\n",
       "      <td>2010-03-03T05:28:02</td>\n",
       "      <td>@USER_77a4822d yea ok..well answer that cheap ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>USER_79321756</td>\n",
       "      <td>2010-03-03T05:56:13</td>\n",
       "      <td>A sprite can disappear in her mouth - lil kim ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id           created_at  \\\n",
       "0  USER_79321756  2010-03-03T04:15:26   \n",
       "1  USER_79321756  2010-03-03T04:55:32   \n",
       "2  USER_79321756  2010-03-03T05:13:34   \n",
       "3  USER_79321756  2010-03-03T05:28:02   \n",
       "4  USER_79321756  2010-03-03T05:56:13   \n",
       "\n",
       "                                                text  \n",
       "0  RT @USER_2ff4faca: IF SHE DO IT 1 MORE TIME......  \n",
       "1  @USER_77a4822d @USER_2ff4faca okay:) lol. Sayi...  \n",
       "2  RT @USER_5d4d777a: YOURE A FAG FOR GETTING IN ...  \n",
       "3  @USER_77a4822d yea ok..well answer that cheap ...  \n",
       "4  A sprite can disappear in her mouth - lil kim ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twt_df = pd.read_csv('GeoText/full_text.txt', sep='\\t', header=None)\n",
    "twt_df.columns = ['user_id', 'created_at', 'latlon', 'lat', 'lon', 'text']\n",
    "twt_df.drop(columns=['latlon', 'lat', 'lon'], inplace=True)\n",
    "twt_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_tweets = twt_df[twt_df['user_id'] == twt_df.user_id.unique()[4457]].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_tweets = account_tweets.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_tweets[:, 0] = NP.array([i for i in range(301)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "##### Now, we should preprocess tweets by filtering the text and recording the sentiments of each tweet\n",
    "\n",
    "Output format: ``` [tweet_ID, created_at, raw_text, cleaned_text, sentiment]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(tweet):\n",
    "\n",
    "    # this will replace seeds (as phrases) as unigrams. lack of > lack_of\n",
    "    for seed in all_seeds_raw:\n",
    "        if seed in tweet and \" \" in seed:\n",
    "            tweet = tweet.replace(seed, seed.replace(\" \", \"_\"))\n",
    "\n",
    "    # remove retweet handler\n",
    "    if tweet[:2] == \"RT\":\n",
    "        try:\n",
    "            colon_idx = tweet.index(\":\")\n",
    "            tweet = tweet[colon_idx+2:]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # remove url from tweet\n",
    "    tweet = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', tweet)\n",
    "\n",
    "    # remove non-ascii characters\n",
    "    tweet = filter(lambda x: x in printable, tweet)\n",
    "\n",
    "    # additional preprocessing\n",
    "    tweet = tweet.replace(\"\\n\", \" \").replace(\" https\",\"\").replace(\"http\",\"\")\n",
    "\n",
    "    # remove all mentions in tweet\n",
    "    mentions = re.findall(r\"@\\w+\", tweet)\n",
    "    for mention in mentions:\n",
    "        tweet = tweet.replace(mention, \"\")\n",
    "\n",
    "    # break usernames and hashtags +++++++++++++\n",
    "    for term in re.findall(r\"#\\w+\", tweet):\n",
    "        token = term[1:]\n",
    "\n",
    "        # remove any punctuations from the hashtag and mention\n",
    "        # ex: Troll_Cinema => TrollCinema\n",
    "        token = token.translate(None, ''.join(string.punctuation))\n",
    "\n",
    "        segments = wordsegment.segment(token)\n",
    "        segments = ' '.join(segments)\n",
    "\n",
    "        tweet = tweet.replace(term, segments)\n",
    "\n",
    "    # remove all punctuations from the tweet text\n",
    "    tweet = \"\".join([char for char in tweet if char not in punctuation])\n",
    "\n",
    "    # remove trailing spaces\n",
    "    tweet = tweet.strip()\n",
    "\n",
    "    # remove all tokens in the tweet where the token is\n",
    "    # a stop word or an emoji\n",
    "    tweet = [word.lower() for word in nltk_tok.tokenize(tweet) if word.lower() not in stoplist \n",
    "                and word.lower() not in emojies and len(word) > 1]\n",
    "\n",
    "    tweet = \" \".join(tweet)\n",
    "\n",
    "    # remove numbers\n",
    "    tweet = re.sub(r'[\\d-]+', 'NUM', tweet)\n",
    "    # padding NUM with spaces\n",
    "    tweet = tweet.replace(\"NUM\", \" NUM \")\n",
    "    # remove multiple spaces in tweet text\n",
    "    tweet = re.sub('\\s{2,}', ' ', tweet)\n",
    "\n",
    "    return tweet\n",
    "\n",
    "\n",
    "# input: [Tweet_ID, created_at, text]\n",
    "def preprocess(account_tweets):\n",
    "    \n",
    "    preprocessed_tweets = list()\n",
    "    \n",
    "    for index, tweet in enumerate(account_tweets):\n",
    "        \n",
    "        cleaned_text = preprocess_text(tweet[2])\n",
    "        sent_score = TextBlob(tweet[2].decode('ascii', errors=\"ignore\")).sentiment.polarity        \n",
    "    \n",
    "        # output: [tweet_ID, created_at, raw_text, cleaned_text, sentiment]\n",
    "        preprocessed_tweets.append([tweet[0], tweet[1], tweet[2], cleaned_text, sent_score])\n",
    "        \n",
    "        if index % 100 == 0:\n",
    "            print \".\",\n",
    "\n",
    "    return preprocessed_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nwrim/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:52: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". . . .\n"
     ]
    }
   ],
   "source": [
    "preprocessed_tweets = preprocess(account_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "##### Now, to emulate PHQ-9 questionare, we bucket tweets based on their creation time with a sliding window of 14 days. Each bucket will then be treated as a document when we run LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sliding_buckets_on_time(account_tweets):\n",
    "\n",
    "    size_of_bucket = 14 # days\n",
    "    \n",
    "    # convert list of lists to pandas dataframe\n",
    "    account_tweets = pd.DataFrame(account_tweets, columns=[\"tweet_ID\", \"created_at\", \"raw_text\", \n",
    "                                                           \"cleaned_text\", \"sentiment\"])\n",
    "    \n",
    "    # ensure that Created_at column is of type datetime\n",
    "    account_tweets['created_at'] = pd.to_datetime(account_tweets['created_at'])\n",
    "\n",
    "    min_date = account_tweets.created_at.min()\n",
    "    max_date = account_tweets.created_at.max()\n",
    "    max_date = max_date + datetime.timedelta(days=1)\n",
    "\n",
    "    min_date = min_date.replace(hour=0, minute=0, second=0)\n",
    "    max_date = max_date.replace(hour=0, minute=0, second=0)\n",
    "\n",
    "    new_min = min_date\n",
    "    new_max = min_date + datetime.timedelta(days=size_of_bucket)\n",
    "\n",
    "    # will contain the tweets grouped in buckets\n",
    "    bucketed_tweets = defaultdict(list)\n",
    "    \n",
    "    for index, tweet in account_tweets.iterrows():\n",
    "        bucketed_tweets[0].append(tweet)\n",
    "\n",
    "    return bucketed_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucketed_tweets = build_sliding_buckets_on_time(preprocessed_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "##### Prepare the data for pSSLDA from the bucketed tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_pSSLDA(bucketed_tweets):\n",
    "\n",
    "    texts = list()\n",
    "\n",
    "    # each bucket is hashed on the start and end date\n",
    "    for bucket in bucketed_tweets:\n",
    "\n",
    "        all_bucket_tweets = \"\"\n",
    "\n",
    "        for tweet in bucketed_tweets[bucket]:\n",
    "\n",
    "            try:\n",
    "                all_bucket_tweets += tweet.cleaned_text + \" \"\n",
    "            except:\n",
    "                # some cleaned fields are None. therefore, ignore!\n",
    "                pass\n",
    "\n",
    "        texts.append(all_bucket_tweets.strip().replace(\"\\n\", \"\").split(\" \"))\n",
    "\n",
    "    # assign each word a unique ID\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "    # remove gaps in id sequence after words that were removed\n",
    "    dictionary.compactify()\n",
    "\n",
    "    voc_size = len(list(dictionary.keys()))\n",
    "\n",
    "    # replace token ids with the token text in each doc and return similar arry of tokens and docs\n",
    "    text_as_ids = list()\n",
    "\n",
    "    # to later be the docvec\n",
    "    doc_as_ids = list()\n",
    "\n",
    "    # number of docs here is the number of buckets\n",
    "    number_of_docs = len(bucketed_tweets)\n",
    "\n",
    "    for x in range(number_of_docs):\n",
    "\n",
    "        doc = texts[x]\n",
    "\n",
    "        for token in doc:\n",
    "            text_as_ids.append(dictionary.token2id[token])\n",
    "            doc_as_ids.append(x)\n",
    " \n",
    "    return text_as_ids, doc_as_ids, voc_size, dictionary.token2id, number_of_docs, bucketed_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs for us will be here multiple tweets\n",
    "pSSLDA_input = prepare_data_for_pSSLDA(bucketed_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "##### Run pSSLDA allowing us to seed the LDA topics using our depression lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: topics and signals are used in interchangebly in this code, they both mean the same thing.\n",
    "\n",
    "# calculated the average sentiment of a token based on its occurence in a given set of tweets\n",
    "# terms sentiment is therefore taken from the tweet sentiment not targeted sentiment\n",
    "def get_avg_sentiment(bucketed_tweets, token):\n",
    "\n",
    "    term_tweets_sent_scores = get_tweets_by_term(bucketed_tweets, token)\n",
    "    \n",
    "    score = 0.0\n",
    "    count = 0\n",
    "\n",
    "    for sent_score in term_tweets_sent_scores:\n",
    "         score += float(sent_score)\n",
    "         count+=1\n",
    "\n",
    "    return score/count\n",
    "\n",
    "\n",
    "def get_tweets_by_term(bucketed_tweets, term):\n",
    "\n",
    "    term_tweets_sent_scores = list()\n",
    "\n",
    "    for bucket in bucketed_tweets:\n",
    "        for tweet in bucketed_tweets[bucket]:\n",
    "            try:\n",
    "                if term in tweet.cleaned_text:\n",
    "                    term_tweets_sent_scores.append(tweet.sentiment)\n",
    "            except:\n",
    "                # pass on empty text field\n",
    "                pass\n",
    "\n",
    "    return term_tweets_sent_scores\n",
    "\n",
    "\n",
    "def get_topics_terms(tup):\n",
    "\n",
    "    estphi = tup[0]\n",
    "    W = tup[1]\n",
    "    T = tup[2]\n",
    "    id2token = tup[3]\n",
    "\n",
    "    # This will contain the mappings of each term to each of our topics\n",
    "    # topic1 -> termX, termY ...\n",
    "    topics_dict = defaultdict(defaultdict)\n",
    "\n",
    "    print \"Reading Topics Terms: \"\n",
    "    \n",
    "    # find the topic where each term is part of\n",
    "    # W: vocabulary size\n",
    "    for index in range(W):\n",
    "        # projects one column of the matrix which contains the weight of the term in all of the topics\n",
    "        term_weights = estphi[:,index]\n",
    "\n",
    "        # will contain the largest weight which ->  topic it was assigned to\n",
    "        largest_weight = 0\n",
    "\n",
    "        for weight in term_weights:\n",
    "            if weight > largest_weight:\n",
    "                largest_weight = weight\n",
    "\n",
    "        # this will get the index of the topic with largest weight\n",
    "        term_topic = NP.argwhere(term_weights==largest_weight)[0][0]\n",
    "\n",
    "        topics_dict[term_topic][id2token[index]] = largest_weight\n",
    "\n",
    "        if index % 50 == 0:\n",
    "            print \".\",\n",
    "    \n",
    "    print \"Done Reading Topics Terms\"\n",
    "    \n",
    "    return topics_dict\n",
    "\n",
    "\n",
    "def get_all_terms_sentiments(id2token, w, bucketed_tweets):\n",
    "\n",
    "    seed_term_sentiment = defaultdict(float)\n",
    "\n",
    "    unique_w = list(set(w))\n",
    "\n",
    "    for wi in unique_w:\n",
    "        token = id2token[wi]\n",
    "\n",
    "        if token in seed_terms['signal_1']:\n",
    "            seed_term_sentiment[token] = get_avg_sentiment(bucketed_tweets, token)\n",
    "\n",
    "        elif token in seed_terms['signal_2']:\n",
    "            seed_term_sentiment[token] = get_avg_sentiment(bucketed_tweets, token)\n",
    "\n",
    "        elif token in seed_terms['signal_3']:\n",
    "            seed_term_sentiment[token] = get_avg_sentiment(bucketed_tweets, token)\n",
    "\n",
    "        elif token in seed_terms['signal_4']:\n",
    "            seed_term_sentiment[token] = get_avg_sentiment(bucketed_tweets, token)\n",
    "\n",
    "        elif token in seed_terms['signal_5']:\n",
    "            seed_term_sentiment[token] = get_avg_sentiment(bucketed_tweets, token)\n",
    "\n",
    "        elif token in seed_terms['signal_6']:\n",
    "            seed_term_sentiment[token] = get_avg_sentiment(bucketed_tweets, token)\n",
    "\n",
    "        elif token in seed_terms['signal_7']:\n",
    "            seed_term_sentiment[token] = get_avg_sentiment(bucketed_tweets, token)\n",
    "\n",
    "        elif token in seed_terms['signal_8']:\n",
    "            seed_term_sentiment[token] = get_avg_sentiment(bucketed_tweets, token)\n",
    "\n",
    "        elif token in seed_terms['signal_9']:\n",
    "            seed_term_sentiment[token] = get_avg_sentiment(bucketed_tweets, token)\n",
    "\n",
    "        elif token in seed_terms['signal_10']:\n",
    "            seed_term_sentiment[token] = get_avg_sentiment(bucketed_tweets, token)\n",
    "\n",
    "    return seed_term_sentiment\n",
    "\n",
    "# This is a modified version of the code in https://github.com/davidandrzej/pSSLDA/blob/master/example/example.py\n",
    "def run_pSSLDA(pSSLDA_input, parameters):\n",
    "    \n",
    "    print \"Running ssToT\"\n",
    "\n",
    "    token2id = pSSLDA_input[3]\n",
    "\n",
    "    # number of topics\n",
    "    T = parameters[\"topics_count\"]\n",
    "\n",
    "    (wordvec, docvec, zvec) = ([], [], [])\n",
    "\n",
    "    # vector of words per tweet\n",
    "    wordvec = pSSLDA_input[0]\n",
    "    docvec = pSSLDA_input[1]\n",
    "\n",
    "    # W = vocabulary size\n",
    "    W = pSSLDA_input[2]\n",
    "\n",
    "    (w, d) = (NP.array(wordvec, dtype = NP.int),\n",
    "              NP.array(docvec, dtype = NP.int))\n",
    "\n",
    "    # Create parameters\n",
    "    alpha = NP.ones((1,T)) * 1\n",
    "    beta = NP.ones((T,W)) * 0.01\n",
    "\n",
    "    # How many parallel samplers do we wish to use?\n",
    "    P = 1\n",
    "\n",
    "    # Random number seed\n",
    "    randseed =  random.randint(999,999999)# 194582\n",
    "\n",
    "    # Number of samples to take\n",
    "    numsamp = 500\n",
    "\n",
    "    # Do parallel inference\n",
    "    finalz = infer(w, d, alpha, beta, numsamp, randseed, P)\n",
    "\n",
    "    # number of documents = tweets\n",
    "    D = pSSLDA_input[4]\n",
    "\n",
    "    # Estimate phi and theta\n",
    "    (nw, nd) = FastLDA.countMatrices(w, W, d, D, finalz, T)\n",
    "    (estphi,esttheta) = FastLDA.estPhiTheta(nw, nd, alpha, beta)\n",
    "\n",
    "    # ======================================================================\n",
    "\n",
    "    # swap keys with values in the token2id => id2token\n",
    "    id2token = dict((v,k) for k,v in token2id.iteritems())\n",
    "\n",
    "    seed_term_sentiment = get_all_terms_sentiments(id2token, w, pSSLDA_input[5])\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    \n",
    "    # Now, we add z-labels to *force* words into separate topics\n",
    "    \n",
    "    labelweight = 5.0\n",
    "\n",
    "    label0 = NP.zeros((T,), dtype=NP.float)\n",
    "    label0[0] = labelweight\n",
    "\n",
    "    label1 = NP.zeros((T,), dtype=NP.float)\n",
    "    label1[1] = labelweight\n",
    "\n",
    "    label2 = NP.zeros((T,), dtype=NP.float)\n",
    "    label2[2] = labelweight\n",
    "\n",
    "    label3 = NP.zeros((T,), dtype=NP.float)\n",
    "    label3[3] = labelweight\n",
    "\n",
    "    label4 = NP.zeros((T,), dtype=NP.float)\n",
    "    label4[4] = labelweight\n",
    "\n",
    "    label5 = NP.zeros((T,), dtype=NP.float)\n",
    "    label5[5] = labelweight\n",
    "\n",
    "    label6 = NP.zeros((T,), dtype=NP.float)\n",
    "    label6[6] = labelweight\n",
    "\n",
    "    label7 = NP.zeros((T,), dtype=NP.float)\n",
    "    label7[7] = labelweight\n",
    "\n",
    "    label8 = NP.zeros((T,), dtype=NP.float)\n",
    "    label8[8] = labelweight\n",
    "\n",
    "    label9 = NP.zeros((T,), dtype=NP.float)\n",
    "    label9[9] = labelweight\n",
    "\n",
    "    label10 = NP.zeros((T,), dtype=NP.float)\n",
    "    label10[10] = labelweight\n",
    "\n",
    "    label11 = NP.zeros((T,), dtype=NP.float)\n",
    "    label11[11] = labelweight\n",
    "\n",
    "    # signals ids\n",
    "    corpus_signals = [0,1,2,3,4,5,6,7,8,9]\n",
    "   \n",
    "    seed_terms_per_signal = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    zlabels = []\n",
    "    for wi in w:\n",
    "\n",
    "        token = id2token[wi]\n",
    "\n",
    "        # if the word appears in topic 0\n",
    "        if token in seed_terms['signal_1'] and  seed_term_sentiment[token] <= 0:\n",
    "\n",
    "            zlabels.append(label0)\n",
    "\n",
    "            seed_terms_per_signal['signal_1'][token]+=1\n",
    "\n",
    "            if 0 in corpus_signals:\n",
    "                corpus_signals.remove(0)\n",
    "\n",
    "\n",
    "        elif token in seed_terms['signal_2'] and  seed_term_sentiment[token] <= 0:\n",
    "\n",
    "            zlabels.append(label1)\n",
    "\n",
    "            seed_terms_per_signal['signal_2'][token]+=1\n",
    "\n",
    "            if 1 in corpus_signals:\n",
    "                corpus_signals.remove(1)\n",
    "\n",
    "\n",
    "        elif token in seed_terms['signal_3'] and seed_term_sentiment[token] <= 0:\n",
    "\n",
    "            zlabels.append(label2)\n",
    "\n",
    "            seed_terms_per_signal['signal_3'][token]+=1\n",
    "\n",
    "            if 2 in corpus_signals:\n",
    "                corpus_signals.remove(2)\n",
    "\n",
    "\n",
    "        elif token in seed_terms['signal_4'] and seed_term_sentiment[token] <= 0:\n",
    "\n",
    "            zlabels.append(label3)\n",
    "            seed_terms_per_signal['signal_4'][token]+=1\n",
    "\n",
    "            if 3 in corpus_signals:\n",
    "                corpus_signals.remove(3)\n",
    "\n",
    "\n",
    "        elif token in seed_terms['signal_5'] and seed_term_sentiment[token] <= 0:\n",
    "\n",
    "            zlabels.append(label4)\n",
    "\n",
    "            seed_terms_per_signal['signal_5'][token]+=1\n",
    "\n",
    "            if 4 in corpus_signals:\n",
    "                corpus_signals.remove(4)\n",
    "\n",
    "        elif token in seed_terms['signal_6'] and  seed_term_sentiment[token] <= 0:\n",
    "\n",
    "            zlabels.append(label5)\n",
    "\n",
    "            seed_terms_per_signal['signal_6'][token]+=1\n",
    "\n",
    "            if 5 in corpus_signals:\n",
    "                corpus_signals.remove(5)\n",
    "\n",
    "        elif token in seed_terms['signal_7'] and  seed_term_sentiment[token] <= 0:\n",
    "\n",
    "            zlabels.append(label6)\n",
    "\n",
    "            seed_terms_per_signal['signal_7'][token]+=1\n",
    "\n",
    "            if 6 in corpus_signals:\n",
    "                corpus_signals.remove(6)\n",
    "\n",
    "        elif token in seed_terms['signal_8'] and  seed_term_sentiment[token] <= 0:\n",
    "\n",
    "            zlabels.append(label7)\n",
    "\n",
    "            seed_terms_per_signal['signal_8'][token]+=1\n",
    "\n",
    "            if 7 in corpus_signals:\n",
    "                corpus_signals.remove(7)\n",
    "\n",
    "        elif token in seed_terms['signal_9'] and  seed_term_sentiment[token] <= 0:\n",
    "\n",
    "            zlabels.append(label8)\n",
    "\n",
    "            seed_terms_per_signal['signal_9'][token]+=1\n",
    "\n",
    "            if 8 in corpus_signals:\n",
    "                corpus_signals.remove(8)\n",
    "\n",
    "        elif token in seed_terms['signal_10'] and  seed_term_sentiment[token] <= 0:\n",
    "\n",
    "            zlabels.append(label9)\n",
    "\n",
    "            seed_terms_per_signal['signal_10'][token]+=1\n",
    "\n",
    "            if 9 in corpus_signals:\n",
    "                corpus_signals.remove(9)\n",
    "\n",
    "        else:\n",
    "            zlabels.append(None)\n",
    "\n",
    "\n",
    "    # --------------------------------------------------------------------\n",
    "\n",
    "    # Now inference will find topics with 0 and 1 in separate topics\n",
    "    finalz = infer(w, d, alpha, beta, numsamp, randseed, P, zlabels = zlabels)\n",
    "\n",
    "    # Re-estimate phi and theta\n",
    "    (nw, nd) = FastLDA.countMatrices(w, W, d, D, finalz, T)\n",
    "    (estphi,esttheta) = FastLDA.estPhiTheta(nw, nd, alpha, beta)\n",
    "\n",
    "    # --------------------------------------------------------------------\n",
    "    \n",
    "    # Find the sentiment of each topic cluster based on the tweets where each seed term appered in\n",
    "\n",
    "    tup = (estphi, W, T, id2token)\n",
    "    topics_terms = get_topics_terms(tup)\n",
    "    \n",
    "    # --------------------------------------------------------------------\n",
    "    \n",
    "    # TODO: refactor this subroutine to make it faster, use inverted index!\n",
    "    \n",
    "    sent_scores = defaultdict(list)\n",
    "\n",
    "    print \"Calculating topics sentiments: \"\n",
    "    \n",
    "    counter = 0\n",
    "    for topic in topics_terms:\n",
    "\n",
    "        topic_sent_scores = list()\n",
    "\n",
    "        for term in topics_terms[topic]:\n",
    "            term_tweets_sent_scores = get_tweets_by_term(pSSLDA_input[5], term)\n",
    "\n",
    "            for sent_score in term_tweets_sent_scores:\n",
    "                 topic_sent_scores.append(float(sent_score))\n",
    "\n",
    "        avg = sum(topic_sent_scores) / float(len(topic_sent_scores))\n",
    "\n",
    "        sent_scores[topic] = (topic_sent_scores, avg)\n",
    "        \n",
    "        counter+=1\n",
    "        print \".\",\n",
    "\n",
    "    # --------------------------------------------------------------------\n",
    "        \n",
    "    # post processing of topics. If the bucket has less than 30 tweets then\n",
    "    # discard the probabilities of that bucket\n",
    "\n",
    "    len_buckets = []\n",
    "    for bucket in pSSLDA_input[5]:\n",
    "        len_b = len(pSSLDA_input[5][bucket])\n",
    "        len_buckets.append(len_b)\n",
    "\n",
    "   \n",
    "    # threshold #1: if number of tweets in that bucket is less than x, then discard that bucket.\n",
    "    min_number_of_tweets_per_bucket = parameters[\"min_tweets_per_bucket\"]\n",
    "    \n",
    "    for x in range(len(len_buckets)):\n",
    "        if len_buckets[x] <= min_number_of_tweets_per_bucket:\n",
    "            esttheta[x, :] = 0\n",
    "\n",
    "    # this will replace zero to the probabilities of the topic by ID if no seed terms were found in the corpus\n",
    "    for topic_id in corpus_signals:\n",
    "        esttheta[:, topic_id] = 0\n",
    "\n",
    "    all_topics_seeds = list()\n",
    "    for signal in seed_terms_per_signal:\n",
    "        all_topics_seeds += seed_terms_per_signal[signal]\n",
    "\n",
    "    # topics to keep\n",
    "    seeds_in_top_k = defaultdict(int)\n",
    "\n",
    "    # number of seed terms that should be in the top topic terms\n",
    "    seeds_threshold = parameters[\"seeds_threshold\"]\n",
    "    # The number of terms in the topic that we will look into to search for seed terms\n",
    "    top_topic_terms = parameters[\"top_topic_terms\"]\n",
    "\n",
    "    for topic in topics_terms:\n",
    "        for x in range(len(topics_terms[topic])):\n",
    "            term = list(topics_terms[topic])[x]\n",
    "            if x < top_topic_terms:\n",
    "                if term in all_topics_seeds:\n",
    "                    seeds_in_top_k[topic] += 1\n",
    "\n",
    "    # this will replace zero to the probabilities of the topic by ID if no seed terms were found in the corpus\n",
    "    for x in range(len(esttheta[0])):\n",
    "        if x in seeds_in_top_k.keys():\n",
    "            if seeds_in_top_k[x] < seeds_threshold:\n",
    "                esttheta[:, x] = 0\n",
    "        else:\n",
    "            esttheta[:, x] = 0\n",
    "\n",
    "\n",
    "    return (estphi, W, T, id2token), esttheta, topics_terms, seed_terms_per_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ssToT\n",
      "Online z initialization\n",
      "Assigning documents to partitions\n",
      "Getting indices associated with each partition\n",
      "Create re-numbered doc vectors for each partition\n",
      "Initializing count matrices\n",
      "Launching Sampler processes\n",
      "Computing global nw count matrix\n",
      "Sample 0 of 500\n",
      "Sample 1 of 500\n",
      "Sample 2 of 500\n",
      "Sample 3 of 500\n",
      "Sample 4 of 500\n",
      "Sample 5 of 500\n",
      "Sample 6 of 500\n",
      "Sample 7 of 500\n",
      "Sample 8 of 500\n",
      "Sample 9 of 500\n",
      "Sample 10 of 500\n",
      "Sample 11 of 500\n",
      "Sample 12 of 500\n",
      "Sample 13 of 500\n",
      "Sample 14 of 500\n",
      "Sample 15 of 500\n",
      "Sample 16 of 500\n",
      "Sample 17 of 500\n",
      "Sample 18 of 500\n",
      "Sample 19 of 500\n",
      "Sample 20 of 500\n",
      "Sample 21 of 500\n",
      "Sample 22 of 500\n",
      "Sample 23 of 500\n",
      "Sample 24 of 500\n",
      "Sample 25 of 500\n",
      "Sample 26 of 500\n",
      "Sample 27 of 500\n",
      "Sample 28 of 500\n",
      "Sample 29 of 500\n",
      "Sample 30 of 500\n",
      "Sample 31 of 500\n",
      "Sample 32 of 500\n",
      "Sample 33 of 500\n",
      "Sample 34 of 500\n",
      "Sample 35 of 500\n",
      "Sample 36 of 500\n",
      "Sample 37 of 500\n",
      "Sample 38 of 500\n",
      "Sample 39 of 500\n",
      "Sample 40 of 500\n",
      "Sample 41 of 500\n",
      "Sample 42 of 500\n",
      "Sample 43 of 500\n",
      "Sample 44 of 500\n",
      "Sample 45 of 500\n",
      "Sample 46 of 500\n",
      "Sample 47 of 500\n",
      "Sample 48 of 500\n",
      "Sample 49 of 500\n",
      "Sample 50 of 500\n",
      "Sample 51 of 500\n",
      "Sample 52 of 500\n",
      "Sample 53 of 500\n",
      "Sample 54 of 500\n",
      "Sample 55 of 500\n",
      "Sample 56 of 500\n",
      "Sample 57 of 500\n",
      "Sample 58 of 500\n",
      "Sample 59 of 500\n",
      "Sample 60 of 500\n",
      "Sample 61 of 500\n",
      "Sample 62 of 500\n",
      "Sample 63 of 500\n",
      "Sample 64 of 500\n",
      "Sample 65 of 500\n",
      "Sample 66 of 500\n",
      "Sample 67 of 500\n",
      "Sample 68 of 500\n",
      "Sample 69 of 500\n",
      "Sample 70 of 500\n",
      "Sample 71 of 500\n",
      "Sample 72 of 500\n",
      "Sample 73 of 500\n",
      "Sample 74 of 500\n",
      "Sample 75 of 500\n",
      "Sample 76 of 500\n",
      "Sample 77 of 500\n",
      "Sample 78 of 500\n",
      "Sample 79 of 500\n",
      "Sample 80 of 500\n",
      "Sample 81 of 500\n",
      "Sample 82 of 500\n",
      "Sample 83 of 500\n",
      "Sample 84 of 500\n",
      "Sample 85 of 500\n",
      "Sample 86 of 500\n",
      "Sample 87 of 500\n",
      "Sample 88 of 500\n",
      "Sample 89 of 500\n",
      "Sample 90 of 500\n",
      "Sample 91 of 500\n",
      "Sample 92 of 500\n",
      "Sample 93 of 500\n",
      "Sample 94 of 500\n",
      "Sample 95 of 500\n",
      "Sample 96 of 500\n",
      "Sample 97 of 500\n",
      "Sample 98 of 500\n",
      "Sample 99 of 500\n",
      "Sample 100 of 500\n",
      "Sample 101 of 500\n",
      "Sample 102 of 500\n",
      "Sample 103 of 500\n",
      "Sample 104 of 500\n",
      "Sample 105 of 500\n",
      "Sample 106 of 500\n",
      "Sample 107 of 500\n",
      "Sample 108 of 500\n",
      "Sample 109 of 500\n",
      "Sample 110 of 500\n",
      "Sample 111 of 500\n",
      "Sample 112 of 500\n",
      "Sample 113 of 500\n",
      "Sample 114 of 500\n",
      "Sample 115 of 500\n",
      "Sample 116 of 500\n",
      "Sample 117 of 500\n",
      "Sample 118 of 500\n",
      "Sample 119 of 500\n",
      "Sample 120 of 500\n",
      "Sample 121 of 500\n",
      "Sample 122 of 500\n",
      "Sample 123 of 500\n",
      "Sample 124 of 500\n",
      "Sample 125 of 500\n",
      "Sample 126 of 500\n",
      "Sample 127 of 500\n",
      "Sample 128 of 500\n",
      "Sample 129 of 500\n",
      "Sample 130 of 500\n",
      "Sample 131 of 500\n",
      "Sample 132 of 500\n",
      "Sample 133 of 500\n",
      "Sample 134 of 500\n",
      "Sample 135 of 500\n",
      "Sample 136 of 500\n",
      "Sample 137 of 500\n",
      "Sample 138 of 500\n",
      "Sample 139 of 500\n",
      "Sample 140 of 500\n",
      "Sample 141 of 500\n",
      "Sample 142 of 500\n",
      "Sample 143 of 500\n",
      "Sample 144 of 500\n",
      "Sample 145 of 500\n",
      "Sample 146 of 500\n",
      "Sample 147 of 500\n",
      "Sample 148 of 500\n",
      "Sample 149 of 500\n",
      "Sample 150 of 500\n",
      "Sample 151 of 500\n",
      "Sample 152 of 500\n",
      "Sample 153 of 500\n",
      "Sample 154 of 500\n",
      "Sample 155 of 500\n",
      "Sample 156 of 500\n",
      "Sample 157 of 500\n",
      "Sample 158 of 500\n",
      "Sample 159 of 500\n",
      "Sample 160 of 500\n",
      "Sample 161 of 500\n",
      "Sample 162 of 500\n",
      "Sample 163 of 500\n",
      "Sample 164 of 500\n",
      "Sample 165 of 500\n",
      "Sample 166 of 500\n",
      "Sample 167 of 500\n",
      "Sample 168 of 500\n",
      "Sample 169 of 500\n",
      "Sample 170 of 500\n",
      "Sample 171 of 500\n",
      "Sample 172 of 500\n",
      "Sample 173 of 500\n",
      "Sample 174 of 500\n",
      "Sample 175 of 500\n",
      "Sample 176 of 500\n",
      "Sample 177 of 500\n",
      "Sample 178 of 500\n",
      "Sample 179 of 500\n",
      "Sample 180 of 500\n",
      "Sample 181 of 500\n",
      "Sample 182 of 500\n",
      "Sample 183 of 500\n",
      "Sample 184 of 500\n",
      "Sample 185 of 500\n",
      "Sample 186 of 500\n",
      "Sample 187 of 500\n",
      "Sample 188 of 500\n",
      "Sample 189 of 500\n",
      "Sample 190 of 500\n",
      "Sample 191 of 500\n",
      "Sample 192 of 500\n",
      "Sample 193 of 500\n",
      "Sample 194 of 500\n",
      "Sample 195 of 500\n",
      "Sample 196 of 500\n",
      "Sample 197 of 500\n",
      "Sample 198 of 500\n",
      "Sample 199 of 500\n",
      "Sample 200 of 500\n",
      "Sample 201 of 500\n",
      "Sample 202 of 500\n",
      "Sample 203 of 500\n",
      "Sample 204 of 500\n",
      "Sample 205 of 500\n",
      "Sample 206 of 500\n",
      "Sample 207 of 500\n",
      "Sample 208 of 500\n",
      "Sample 209 of 500\n",
      "Sample 210 of 500\n",
      "Sample 211 of 500\n",
      "Sample 212 of 500\n",
      "Sample 213 of 500\n",
      "Sample 214 of 500\n",
      "Sample 215 of 500\n",
      "Sample 216 of 500\n",
      "Sample 217 of 500\n",
      "Sample 218 of 500\n",
      "Sample 219 of 500\n",
      "Sample 220 of 500\n",
      "Sample 221 of 500\n",
      "Sample 222 of 500\n",
      "Sample 223 of 500\n",
      "Sample 224 of 500\n",
      "Sample 225 of 500\n",
      "Sample 226 of 500\n",
      "Sample 227 of 500\n",
      "Sample 228 of 500\n",
      "Sample 229 of 500\n",
      "Sample 230 of 500\n",
      "Sample 231 of 500\n",
      "Sample 232 of 500\n",
      "Sample 233 of 500\n",
      "Sample 234 of 500\n",
      "Sample 235 of 500\n",
      "Sample 236 of 500\n",
      "Sample 237 of 500\n",
      "Sample 238 of 500\n",
      "Sample 239 of 500\n",
      "Sample 240 of 500\n",
      "Sample 241 of 500\n",
      "Sample 242 of 500\n",
      "Sample 243 of 500\n",
      "Sample 244 of 500\n",
      "Sample 245 of 500\n",
      "Sample 246 of 500\n",
      "Sample 247 of 500\n",
      "Sample 248 of 500\n",
      "Sample 249 of 500\n",
      "Sample 250 of 500\n",
      "Sample 251 of 500\n",
      "Sample 252 of 500\n",
      "Sample 253 of 500\n",
      "Sample 254 of 500\n",
      "Sample 255 of 500\n",
      "Sample 256 of 500\n",
      "Sample 257 of 500\n",
      "Sample 258 of 500\n",
      "Sample 259 of 500\n",
      "Sample 260 of 500\n",
      "Sample 261 of 500\n",
      "Sample 262 of 500\n",
      "Sample 263 of 500\n",
      "Sample 264 of 500\n",
      "Sample 265 of 500\n",
      "Sample 266 of 500\n",
      "Sample 267 of 500\n",
      "Sample 268 of 500\n",
      "Sample 269 of 500\n",
      "Sample 270 of 500\n",
      "Sample 271 of 500\n",
      "Sample 272 of 500\n",
      "Sample 273 of 500\n",
      "Sample 274 of 500\n",
      "Sample 275 of 500\n",
      "Sample 276 of 500\n",
      "Sample 277 of 500\n",
      "Sample 278 of 500\n",
      "Sample 279 of 500\n",
      "Sample 280 of 500\n",
      "Sample 281 of 500\n",
      "Sample 282 of 500\n",
      "Sample 283 of 500\n",
      "Sample 284 of 500\n",
      "Sample 285 of 500\n",
      "Sample 286 of 500\n",
      "Sample 287 of 500\n",
      "Sample 288 of 500\n",
      "Sample 289 of 500\n",
      "Sample 290 of 500\n",
      "Sample 291 of 500\n",
      "Sample 292 of 500\n",
      "Sample 293 of 500\n",
      "Sample 294 of 500\n",
      "Sample 295 of 500\n",
      "Sample 296 of 500\n",
      "Sample 297 of 500\n",
      "Sample 298 of 500\n",
      "Sample 299 of 500\n",
      "Sample 300 of 500\n",
      "Sample 301 of 500\n",
      "Sample 302 of 500\n",
      "Sample 303 of 500\n",
      "Sample 304 of 500\n",
      "Sample 305 of 500\n",
      "Sample 306 of 500\n",
      "Sample 307 of 500\n",
      "Sample 308 of 500\n",
      "Sample 309 of 500\n",
      "Sample 310 of 500\n",
      "Sample 311 of 500\n",
      "Sample 312 of 500\n",
      "Sample 313 of 500\n",
      "Sample 314 of 500\n",
      "Sample 315 of 500\n",
      "Sample 316 of 500\n",
      "Sample 317 of 500\n",
      "Sample 318 of 500\n",
      "Sample 319 of 500\n",
      "Sample 320 of 500\n",
      "Sample 321 of 500\n",
      "Sample 322 of 500\n",
      "Sample 323 of 500\n",
      "Sample 324 of 500\n",
      "Sample 325 of 500\n",
      "Sample 326 of 500\n",
      "Sample 327 of 500\n",
      "Sample 328 of 500\n",
      "Sample 329 of 500\n",
      "Sample 330 of 500\n",
      "Sample 331 of 500\n",
      "Sample 332 of 500\n",
      "Sample 333 of 500\n",
      "Sample 334 of 500\n",
      "Sample 335 of 500\n",
      "Sample 336 of 500\n",
      "Sample 337 of 500\n",
      "Sample 338 of 500\n",
      "Sample 339 of 500\n",
      "Sample 340 of 500\n",
      "Sample 341 of 500\n",
      "Sample 342 of 500\n",
      "Sample 343 of 500\n",
      "Sample 344 of 500\n",
      "Sample 345 of 500\n",
      "Sample 346 of 500\n",
      "Sample 347 of 500\n",
      "Sample 348 of 500\n",
      "Sample 349 of 500\n",
      "Sample 350 of 500\n",
      "Sample 351 of 500\n",
      "Sample 352 of 500\n",
      "Sample 353 of 500\n",
      "Sample 354 of 500\n",
      "Sample 355 of 500\n",
      "Sample 356 of 500\n",
      "Sample 357 of 500\n",
      "Sample 358 of 500\n",
      "Sample 359 of 500\n",
      "Sample 360 of 500\n",
      "Sample 361 of 500\n",
      "Sample 362 of 500\n",
      "Sample 363 of 500\n",
      "Sample 364 of 500\n",
      "Sample 365 of 500\n",
      "Sample 366 of 500\n",
      "Sample 367 of 500\n",
      "Sample 368 of 500\n",
      "Sample 369 of 500\n",
      "Sample 370 of 500\n",
      "Sample 371 of 500\n",
      "Sample 372 of 500\n",
      "Sample 373 of 500\n",
      "Sample 374 of 500\n",
      "Sample 375 of 500\n",
      "Sample 376 of 500\n",
      "Sample 377 of 500\n",
      "Sample 378 of 500\n",
      "Sample 379 of 500\n",
      "Sample 380 of 500\n",
      "Sample 381 of 500\n",
      "Sample 382 of 500\n",
      "Sample 383 of 500\n",
      "Sample 384 of 500\n",
      "Sample 385 of 500\n",
      "Sample 386 of 500\n",
      "Sample 387 of 500\n",
      "Sample 388 of 500\n",
      "Sample 389 of 500\n",
      "Sample 390 of 500\n",
      "Sample 391 of 500\n",
      "Sample 392 of 500\n",
      "Sample 393 of 500\n",
      "Sample 394 of 500\n",
      "Sample 395 of 500\n",
      "Sample 396 of 500\n",
      "Sample 397 of 500\n",
      "Sample 398 of 500\n",
      "Sample 399 of 500\n",
      "Sample 400 of 500\n",
      "Sample 401 of 500\n",
      "Sample 402 of 500\n",
      "Sample 403 of 500\n",
      "Sample 404 of 500\n",
      "Sample 405 of 500\n",
      "Sample 406 of 500\n",
      "Sample 407 of 500\n",
      "Sample 408 of 500\n",
      "Sample 409 of 500\n",
      "Sample 410 of 500\n",
      "Sample 411 of 500\n",
      "Sample 412 of 500\n",
      "Sample 413 of 500\n",
      "Sample 414 of 500\n",
      "Sample 415 of 500\n",
      "Sample 416 of 500\n",
      "Sample 417 of 500\n",
      "Sample 418 of 500\n",
      "Sample 419 of 500\n",
      "Sample 420 of 500\n",
      "Sample 421 of 500\n",
      "Sample 422 of 500\n",
      "Sample 423 of 500\n",
      "Sample 424 of 500\n",
      "Sample 425 of 500\n",
      "Sample 426 of 500\n",
      "Sample 427 of 500\n",
      "Sample 428 of 500\n",
      "Sample 429 of 500\n",
      "Sample 430 of 500\n",
      "Sample 431 of 500\n",
      "Sample 432 of 500\n",
      "Sample 433 of 500\n",
      "Sample 434 of 500\n",
      "Sample 435 of 500\n",
      "Sample 436 of 500\n",
      "Sample 437 of 500\n",
      "Sample 438 of 500\n",
      "Sample 439 of 500\n",
      "Sample 440 of 500\n",
      "Sample 441 of 500\n",
      "Sample 442 of 500\n",
      "Sample 443 of 500\n",
      "Sample 444 of 500\n",
      "Sample 445 of 500\n",
      "Sample 446 of 500\n",
      "Sample 447 of 500\n",
      "Sample 448 of 500\n",
      "Sample 449 of 500\n",
      "Sample 450 of 500\n",
      "Sample 451 of 500\n",
      "Sample 452 of 500\n",
      "Sample 453 of 500\n",
      "Sample 454 of 500\n",
      "Sample 455 of 500\n",
      "Sample 456 of 500\n",
      "Sample 457 of 500\n",
      "Sample 458 of 500\n",
      "Sample 459 of 500\n",
      "Sample 460 of 500\n",
      "Sample 461 of 500\n",
      "Sample 462 of 500\n",
      "Sample 463 of 500\n",
      "Sample 464 of 500\n",
      "Sample 465 of 500\n",
      "Sample 466 of 500\n",
      "Sample 467 of 500\n",
      "Sample 468 of 500\n",
      "Sample 469 of 500\n",
      "Sample 470 of 500\n",
      "Sample 471 of 500\n",
      "Sample 472 of 500\n",
      "Sample 473 of 500\n",
      "Sample 474 of 500\n",
      "Sample 475 of 500\n",
      "Sample 476 of 500\n",
      "Sample 477 of 500\n",
      "Sample 478 of 500\n",
      "Sample 479 of 500\n",
      "Sample 480 of 500\n",
      "Sample 481 of 500\n",
      "Sample 482 of 500\n",
      "Sample 483 of 500\n",
      "Sample 484 of 500\n",
      "Sample 485 of 500\n",
      "Sample 486 of 500\n",
      "Sample 487 of 500\n",
      "Sample 488 of 500\n",
      "Sample 489 of 500\n",
      "Sample 490 of 500\n",
      "Sample 491 of 500\n",
      "Sample 492 of 500\n",
      "Sample 493 of 500\n",
      "Sample 494 of 500\n",
      "Sample 495 of 500\n",
      "Sample 496 of 500\n",
      "Sample 497 of 500\n",
      "Sample 498 of 500\n",
      "Sample 499 of 500\n",
      "Online z initialization\n",
      "Assigning documents to partitions\n",
      "Getting indices associated with each partition\n",
      "Create re-numbered doc vectors for each partition\n",
      "Initializing count matrices\n",
      "Launching Sampler processes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing global nw count matrix\n",
      "Sample 0 of 500\n",
      "Sample 1 of 500\n",
      "Sample 2 of 500\n",
      "Sample 3 of 500\n",
      "Sample 4 of 500\n",
      "Sample 5 of 500\n",
      "Sample 6 of 500\n",
      "Sample 7 of 500\n",
      "Sample 8 of 500\n",
      "Sample 9 of 500\n",
      "Sample 10 of 500\n",
      "Sample 11 of 500\n",
      "Sample 12 of 500\n",
      "Sample 13 of 500\n",
      "Sample 14 of 500\n",
      "Sample 15 of 500\n",
      "Sample 16 of 500\n",
      "Sample 17 of 500\n",
      "Sample 18 of 500\n",
      "Sample 19 of 500\n",
      "Sample 20 of 500\n",
      "Sample 21 of 500\n",
      "Sample 22 of 500\n",
      "Sample 23 of 500\n",
      "Sample 24 of 500\n",
      "Sample 25 of 500\n",
      "Sample 26 of 500\n",
      "Sample 27 of 500\n",
      "Sample 28 of 500\n",
      "Sample 29 of 500\n",
      "Sample 30 of 500\n",
      "Sample 31 of 500\n",
      "Sample 32 of 500\n",
      "Sample 33 of 500\n",
      "Sample 34 of 500\n",
      "Sample 35 of 500\n",
      "Sample 36 of 500\n",
      "Sample 37 of 500\n",
      "Sample 38 of 500\n",
      "Sample 39 of 500\n",
      "Sample 40 of 500\n",
      "Sample 41 of 500\n",
      "Sample 42 of 500\n",
      "Sample 43 of 500\n",
      "Sample 44 of 500\n",
      "Sample 45 of 500\n",
      "Sample 46 of 500\n",
      "Sample 47 of 500\n",
      "Sample 48 of 500\n",
      "Sample 49 of 500\n",
      "Sample 50 of 500\n",
      "Sample 51 of 500\n",
      "Sample 52 of 500\n",
      "Sample 53 of 500\n",
      "Sample 54 of 500\n",
      "Sample 55 of 500\n",
      "Sample 56 of 500\n",
      "Sample 57 of 500\n",
      "Sample 58 of 500\n",
      "Sample 59 of 500\n",
      "Sample 60 of 500\n",
      "Sample 61 of 500\n",
      "Sample 62 of 500\n",
      "Sample 63 of 500\n",
      "Sample 64 of 500\n",
      "Sample 65 of 500\n",
      "Sample 66 of 500\n",
      "Sample 67 of 500\n",
      "Sample 68 of 500\n",
      "Sample 69 of 500\n",
      "Sample 70 of 500\n",
      "Sample 71 of 500\n",
      "Sample 72 of 500\n",
      "Sample 73 of 500\n",
      "Sample 74 of 500\n",
      "Sample 75 of 500\n",
      "Sample 76 of 500\n",
      "Sample 77 of 500\n",
      "Sample 78 of 500\n",
      "Sample 79 of 500\n",
      "Sample 80 of 500\n",
      "Sample 81 of 500\n",
      "Sample 82 of 500\n",
      "Sample 83 of 500\n",
      "Sample 84 of 500\n",
      "Sample 85 of 500\n",
      "Sample 86 of 500\n",
      "Sample 87 of 500\n",
      "Sample 88 of 500\n",
      "Sample 89 of 500\n",
      "Sample 90 of 500\n",
      "Sample 91 of 500\n",
      "Sample 92 of 500\n",
      "Sample 93 of 500\n",
      "Sample 94 of 500\n",
      "Sample 95 of 500\n",
      "Sample 96 of 500\n",
      "Sample 97 of 500\n",
      "Sample 98 of 500\n",
      "Sample 99 of 500\n",
      "Sample 100 of 500\n",
      "Sample 101 of 500\n",
      "Sample 102 of 500\n",
      "Sample 103 of 500\n",
      "Sample 104 of 500\n",
      "Sample 105 of 500\n",
      "Sample 106 of 500\n",
      "Sample 107 of 500\n",
      "Sample 108 of 500\n",
      "Sample 109 of 500\n",
      "Sample 110 of 500\n",
      "Sample 111 of 500\n",
      "Sample 112 of 500\n",
      "Sample 113 of 500\n",
      "Sample 114 of 500\n",
      "Sample 115 of 500\n",
      "Sample 116 of 500\n",
      "Sample 117 of 500\n",
      "Sample 118 of 500\n",
      "Sample 119 of 500\n",
      "Sample 120 of 500\n",
      "Sample 121 of 500\n",
      "Sample 122 of 500\n",
      "Sample 123 of 500\n",
      "Sample 124 of 500\n",
      "Sample 125 of 500\n",
      "Sample 126 of 500\n",
      "Sample 127 of 500\n",
      "Sample 128 of 500\n",
      "Sample 129 of 500\n",
      "Sample 130 of 500\n",
      "Sample 131 of 500\n",
      "Sample 132 of 500\n",
      "Sample 133 of 500\n",
      "Sample 134 of 500\n",
      "Sample 135 of 500\n",
      "Sample 136 of 500\n",
      "Sample 137 of 500\n",
      "Sample 138 of 500\n",
      "Sample 139 of 500\n",
      "Sample 140 of 500\n",
      "Sample 141 of 500\n",
      "Sample 142 of 500\n",
      "Sample 143 of 500\n",
      "Sample 144 of 500\n",
      "Sample 145 of 500\n",
      "Sample 146 of 500\n",
      "Sample 147 of 500\n",
      "Sample 148 of 500\n",
      "Sample 149 of 500\n",
      "Sample 150 of 500\n",
      "Sample 151 of 500\n",
      "Sample 152 of 500\n",
      "Sample 153 of 500\n",
      "Sample 154 of 500\n",
      "Sample 155 of 500\n",
      "Sample 156 of 500\n",
      "Sample 157 of 500\n",
      "Sample 158 of 500\n",
      "Sample 159 of 500\n",
      "Sample 160 of 500\n",
      "Sample 161 of 500\n",
      "Sample 162 of 500\n",
      "Sample 163 of 500\n",
      "Sample 164 of 500\n",
      "Sample 165 of 500\n",
      "Sample 166 of 500\n",
      "Sample 167 of 500\n",
      "Sample 168 of 500\n",
      "Sample 169 of 500\n",
      "Sample 170 of 500\n",
      "Sample 171 of 500\n",
      "Sample 172 of 500\n",
      "Sample 173 of 500\n",
      "Sample 174 of 500\n",
      "Sample 175 of 500\n",
      "Sample 176 of 500\n",
      "Sample 177 of 500\n",
      "Sample 178 of 500\n",
      "Sample 179 of 500\n",
      "Sample 180 of 500\n",
      "Sample 181 of 500\n",
      "Sample 182 of 500\n",
      "Sample 183 of 500\n",
      "Sample 184 of 500\n",
      "Sample 185 of 500\n",
      "Sample 186 of 500\n",
      "Sample 187 of 500\n",
      "Sample 188 of 500\n",
      "Sample 189 of 500\n",
      "Sample 190 of 500\n",
      "Sample 191 of 500\n",
      "Sample 192 of 500\n",
      "Sample 193 of 500\n",
      "Sample 194 of 500\n",
      "Sample 195 of 500\n",
      "Sample 196 of 500\n",
      "Sample 197 of 500\n",
      "Sample 198 of 500\n",
      "Sample 199 of 500\n",
      "Sample 200 of 500\n",
      "Sample 201 of 500\n",
      "Sample 202 of 500\n",
      "Sample 203 of 500\n",
      "Sample 204 of 500\n",
      "Sample 205 of 500\n",
      "Sample 206 of 500\n",
      "Sample 207 of 500\n",
      "Sample 208 of 500\n",
      "Sample 209 of 500\n",
      "Sample 210 of 500\n",
      "Sample 211 of 500\n",
      "Sample 212 of 500\n",
      "Sample 213 of 500\n",
      "Sample 214 of 500\n",
      "Sample 215 of 500\n",
      "Sample 216 of 500\n",
      "Sample 217 of 500\n",
      "Sample 218 of 500\n",
      "Sample 219 of 500\n",
      "Sample 220 of 500\n",
      "Sample 221 of 500\n",
      "Sample 222 of 500\n",
      "Sample 223 of 500\n",
      "Sample 224 of 500\n",
      "Sample 225 of 500\n",
      "Sample 226 of 500\n",
      "Sample 227 of 500\n",
      "Sample 228 of 500\n",
      "Sample 229 of 500\n",
      "Sample 230 of 500\n",
      "Sample 231 of 500\n",
      "Sample 232 of 500\n",
      "Sample 233 of 500\n",
      "Sample 234 of 500\n",
      "Sample 235 of 500\n",
      "Sample 236 of 500\n",
      "Sample 237 of 500\n",
      "Sample 238 of 500\n",
      "Sample 239 of 500\n",
      "Sample 240 of 500\n",
      "Sample 241 of 500\n",
      "Sample 242 of 500\n",
      "Sample 243 of 500\n",
      "Sample 244 of 500\n",
      "Sample 245 of 500\n",
      "Sample 246 of 500\n",
      "Sample 247 of 500\n",
      "Sample 248 of 500\n",
      "Sample 249 of 500\n",
      "Sample 250 of 500\n",
      "Sample 251 of 500\n",
      "Sample 252 of 500\n",
      "Sample 253 of 500\n",
      "Sample 254 of 500\n",
      "Sample 255 of 500\n",
      "Sample 256 of 500\n",
      "Sample 257 of 500\n",
      "Sample 258 of 500\n",
      "Sample 259 of 500\n",
      "Sample 260 of 500\n",
      "Sample 261 of 500\n",
      "Sample 262 of 500\n",
      "Sample 263 of 500\n",
      "Sample 264 of 500\n",
      "Sample 265 of 500\n",
      "Sample 266 of 500\n",
      "Sample 267 of 500\n",
      "Sample 268 of 500\n",
      "Sample 269 of 500\n",
      "Sample 270 of 500\n",
      "Sample 271 of 500\n",
      "Sample 272 of 500\n",
      "Sample 273 of 500\n",
      "Sample 274 of 500\n",
      "Sample 275 of 500\n",
      "Sample 276 of 500\n",
      "Sample 277 of 500\n",
      "Sample 278 of 500\n",
      "Sample 279 of 500\n",
      "Sample 280 of 500\n",
      "Sample 281 of 500\n",
      "Sample 282 of 500\n",
      "Sample 283 of 500\n",
      "Sample 284 of 500\n",
      "Sample 285 of 500\n",
      "Sample 286 of 500\n",
      "Sample 287 of 500\n",
      "Sample 288 of 500\n",
      "Sample 289 of 500\n",
      "Sample 290 of 500\n",
      "Sample 291 of 500\n",
      "Sample 292 of 500\n",
      "Sample 293 of 500\n",
      "Sample 294 of 500\n",
      "Sample 295 of 500\n",
      "Sample 296 of 500\n",
      "Sample 297 of 500\n",
      "Sample 298 of 500\n",
      "Sample 299 of 500\n",
      "Sample 300 of 500\n",
      "Sample 301 of 500\n",
      "Sample 302 of 500\n",
      "Sample 303 of 500\n",
      "Sample 304 of 500\n",
      "Sample 305 of 500\n",
      "Sample 306 of 500\n",
      "Sample 307 of 500\n",
      "Sample 308 of 500\n",
      "Sample 309 of 500\n",
      "Sample 310 of 500\n",
      "Sample 311 of 500\n",
      "Sample 312 of 500\n",
      "Sample 313 of 500\n",
      "Sample 314 of 500\n",
      "Sample 315 of 500\n",
      "Sample 316 of 500\n",
      "Sample 317 of 500\n",
      "Sample 318 of 500\n",
      "Sample 319 of 500\n",
      "Sample 320 of 500\n",
      "Sample 321 of 500\n",
      "Sample 322 of 500\n",
      "Sample 323 of 500\n",
      "Sample 324 of 500\n",
      "Sample 325 of 500\n",
      "Sample 326 of 500\n",
      "Sample 327 of 500\n",
      "Sample 328 of 500\n",
      "Sample 329 of 500\n",
      "Sample 330 of 500\n",
      "Sample 331 of 500\n",
      "Sample 332 of 500\n",
      "Sample 333 of 500\n",
      "Sample 334 of 500\n",
      "Sample 335 of 500\n",
      "Sample 336 of 500\n",
      "Sample 337 of 500\n",
      "Sample 338 of 500\n",
      "Sample 339 of 500\n",
      "Sample 340 of 500\n",
      "Sample 341 of 500\n",
      "Sample 342 of 500\n",
      "Sample 343 of 500\n",
      "Sample 344 of 500\n",
      "Sample 345 of 500\n",
      "Sample 346 of 500\n",
      "Sample 347 of 500\n",
      "Sample 348 of 500\n",
      "Sample 349 of 500\n",
      "Sample 350 of 500\n",
      "Sample 351 of 500\n",
      "Sample 352 of 500\n",
      "Sample 353 of 500\n",
      "Sample 354 of 500\n",
      "Sample 355 of 500\n",
      "Sample 356 of 500\n",
      "Sample 357 of 500\n",
      "Sample 358 of 500\n",
      "Sample 359 of 500\n",
      "Sample 360 of 500\n",
      "Sample 361 of 500\n",
      "Sample 362 of 500\n",
      "Sample 363 of 500\n",
      "Sample 364 of 500\n",
      "Sample 365 of 500\n",
      "Sample 366 of 500\n",
      "Sample 367 of 500\n",
      "Sample 368 of 500\n",
      "Sample 369 of 500\n",
      "Sample 370 of 500\n",
      "Sample 371 of 500\n",
      "Sample 372 of 500\n",
      "Sample 373 of 500\n",
      "Sample 374 of 500\n",
      "Sample 375 of 500\n",
      "Sample 376 of 500\n",
      "Sample 377 of 500\n",
      "Sample 378 of 500\n",
      "Sample 379 of 500\n",
      "Sample 380 of 500\n",
      "Sample 381 of 500\n",
      "Sample 382 of 500\n",
      "Sample 383 of 500\n",
      "Sample 384 of 500\n",
      "Sample 385 of 500\n",
      "Sample 386 of 500\n",
      "Sample 387 of 500\n",
      "Sample 388 of 500\n",
      "Sample 389 of 500\n",
      "Sample 390 of 500\n",
      "Sample 391 of 500\n",
      "Sample 392 of 500\n",
      "Sample 393 of 500\n",
      "Sample 394 of 500\n",
      "Sample 395 of 500\n",
      "Sample 396 of 500\n",
      "Sample 397 of 500\n",
      "Sample 398 of 500\n",
      "Sample 399 of 500\n",
      "Sample 400 of 500\n",
      "Sample 401 of 500\n",
      "Sample 402 of 500\n",
      "Sample 403 of 500\n",
      "Sample 404 of 500\n",
      "Sample 405 of 500\n",
      "Sample 406 of 500\n",
      "Sample 407 of 500\n",
      "Sample 408 of 500\n",
      "Sample 409 of 500\n",
      "Sample 410 of 500\n",
      "Sample 411 of 500\n",
      "Sample 412 of 500\n",
      "Sample 413 of 500\n",
      "Sample 414 of 500\n",
      "Sample 415 of 500\n",
      "Sample 416 of 500\n",
      "Sample 417 of 500\n",
      "Sample 418 of 500\n",
      "Sample 419 of 500\n",
      "Sample 420 of 500\n",
      "Sample 421 of 500\n",
      "Sample 422 of 500\n",
      "Sample 423 of 500\n",
      "Sample 424 of 500\n",
      "Sample 425 of 500\n",
      "Sample 426 of 500\n",
      "Sample 427 of 500\n",
      "Sample 428 of 500\n",
      "Sample 429 of 500\n",
      "Sample 430 of 500\n",
      "Sample 431 of 500\n",
      "Sample 432 of 500\n",
      "Sample 433 of 500\n",
      "Sample 434 of 500\n",
      "Sample 435 of 500\n",
      "Sample 436 of 500\n",
      "Sample 437 of 500\n",
      "Sample 438 of 500\n",
      "Sample 439 of 500\n",
      "Sample 440 of 500\n",
      "Sample 441 of 500\n",
      "Sample 442 of 500\n",
      "Sample 443 of 500\n",
      "Sample 444 of 500\n",
      "Sample 445 of 500\n",
      "Sample 446 of 500\n",
      "Sample 447 of 500\n",
      "Sample 448 of 500\n",
      "Sample 449 of 500\n",
      "Sample 450 of 500\n",
      "Sample 451 of 500\n",
      "Sample 452 of 500\n",
      "Sample 453 of 500\n",
      "Sample 454 of 500\n",
      "Sample 455 of 500\n",
      "Sample 456 of 500\n",
      "Sample 457 of 500\n",
      "Sample 458 of 500\n",
      "Sample 459 of 500\n",
      "Sample 460 of 500\n",
      "Sample 461 of 500\n",
      "Sample 462 of 500\n",
      "Sample 463 of 500\n",
      "Sample 464 of 500\n",
      "Sample 465 of 500\n",
      "Sample 466 of 500\n",
      "Sample 467 of 500\n",
      "Sample 468 of 500\n",
      "Sample 469 of 500\n",
      "Sample 470 of 500\n",
      "Sample 471 of 500\n",
      "Sample 472 of 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 473 of 500\n",
      "Sample 474 of 500\n",
      "Sample 475 of 500\n",
      "Sample 476 of 500\n",
      "Sample 477 of 500\n",
      "Sample 478 of 500\n",
      "Sample 479 of 500\n",
      "Sample 480 of 500\n",
      "Sample 481 of 500\n",
      "Sample 482 of 500\n",
      "Sample 483 of 500\n",
      "Sample 484 of 500\n",
      "Sample 485 of 500\n",
      "Sample 486 of 500\n",
      "Sample 487 of 500\n",
      "Sample 488 of 500\n",
      "Sample 489 of 500\n",
      "Sample 490 of 500\n",
      "Sample 491 of 500\n",
      "Sample 492 of 500\n",
      "Sample 493 of 500\n",
      "Sample 494 of 500\n",
      "Sample 495 of 500\n",
      "Sample 496 of 500\n",
      "Sample 497 of 500\n",
      "Sample 498 of 500\n",
      "Sample 499 of 500\n",
      "Reading Topics Terms: \n",
      ". . . . . . . . . . . . . . . Done Reading Topics Terms\n",
      "Calculating topics sentiments: \n",
      ". . . . . . . . . . . . . . .\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "    topics_count: number of topics/signals to construct using pSSLDA\n",
    "    min_tweets_per_bucket: minimum number of tweets per bucket to constructs and accept a topic from it\n",
    "    seeds_threshold: number of seed terms in the top topic terms\n",
    "    top_topic_terms: the number of terms to consider when searching for seed terms\n",
    "'''\n",
    "parameters = {\"topics_count\": 15, \"min_tweets_per_bucket\": 20, \"seeds_threshold\": 2, \"top_topic_terms\": 25}\n",
    "\n",
    "# pSSLDA_input: 0> estphi_tup | 1> esttheta | 2> topics_terms | 3> seed_terms_per_signal\n",
    "pSSLDA_output = run_pSSLDA(pSSLDA_input, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_depression(pSSLDA_output):\n",
    "    \n",
    "    try:\n",
    "\n",
    "        esttheta = pSSLDA_output[1]\n",
    "\n",
    "        print\n",
    "        print \"<<<<<<< Topics Probabilties Over Time >>>>>>>\"\n",
    "        print \n",
    "                \n",
    "        headers = [\"Time Period\", \"Signal-1\", \"Signal-2\", \"Signal-3\", \"Signal-4\", \"Signal-5\",\n",
    "                                  \"Signal-6\", \"Signal-7\", \"Signal-8\", \"Signal-9\", \"Signal-10\"]\n",
    "        \n",
    "        rows = list()\n",
    "        \n",
    "        counter = 0\n",
    "        for key in bucketed_tweets.keys():\n",
    "            \n",
    "            # list of series to dataframe\n",
    "            df = pd.DataFrame(bucketed_tweets[key])\n",
    "                        \n",
    "            bucket_date = str(df.created_at.min().strftime(\"%d/%m/%Y\")) + \" To \" + \\\n",
    "                          str(df.created_at.max().strftime(\"%d/%m/%Y\"))\n",
    "            \n",
    "            row = [bucket_date] + [esttheta[counter][x] for x in range(len(esttheta[counter])) if x < 10]\n",
    "            \n",
    "            rows.append(row)\n",
    "\n",
    "            # increment counter to get element from the result matrix\n",
    "            counter+=1\n",
    "\n",
    "        topics_probabilities = pd.DataFrame(rows, columns=headers)\n",
    "        \n",
    "        print topics_probabilities\n",
    "        \n",
    "        topics_probabilities.plot(kind='line')\n",
    "        plt.show()\n",
    "                \n",
    "        #--------------------------------------------------------------------------------------------\n",
    "\n",
    "        print\n",
    "        print \"<<<<<<< Topics Terms >>>>>>>\"\n",
    "        print \n",
    "\n",
    "        \n",
    "        headers = [\"Topic Number\", \"Topic Terms\"]\n",
    "        rows = list()\n",
    "        \n",
    "        for topic in pSSLDA_output[2]:\n",
    "\n",
    "            topic_nbr = topic+1\n",
    "            \n",
    "            rows.append([topic_nbr, \", \".join(pSSLDA_output[2][topic])])\n",
    "\n",
    "        topics_terms = pd.DataFrame(rows, columns=headers)\n",
    "                    \n",
    "        print topics_terms\n",
    "        \n",
    "        #--------------------------------------------------------------------------------------------\n",
    "\n",
    "        print\n",
    "        print \"<<<<<<< Seeded Terms Per Topic >>>>>>>\"\n",
    "        print\n",
    "\n",
    "        headers = [\"Topic Number\", \"Seed Terms:Count\"]\n",
    "        rows = list()\n",
    "        \n",
    "        # pSSLDA_output[3] = seed_terms_per_signal\n",
    "        for topic in pSSLDA_output[3]:\n",
    "            \n",
    "            seedTerms = [str(seedTerm)+\":\"+str(pSSLDA_output[3][topic][seedTerm]) \n",
    "                                         for seedTerm in pSSLDA_output[3][topic]]\n",
    "            \n",
    "            rows.append([topic, \", \".join(seedTerms)])\n",
    "        \n",
    "        topics_seeds = pd.DataFrame(rows, columns=headers)\n",
    "                    \n",
    "        print topics_seeds\n",
    "\n",
    "    except AssertionError:\n",
    "        print \"ERROR: number of tweets is insufficents for depression detection!\"\n",
    "    except Exception as e:\n",
    "        print \"ERROR >>> \", e\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<<<<<<< Topics Probabilties Over Time >>>>>>>\n",
      "\n",
      "                Time Period  Signal-1  Signal-2  Signal-3  Signal-4  Signal-5  \\\n",
      "0  02/03/2010 To 07/03/2010       0.0       0.0       0.0       0.0       0.0   \n",
      "\n",
      "   Signal-6  Signal-7  Signal-8  Signal-9  Signal-10  \n",
      "0       0.0       0.0       0.0  0.060811        0.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nwrim/anaconda2/lib/python2.7/site-packages/matplotlib/axes/_base.py:3152: UserWarning: Attempting to set identical left==right results\n",
      "in singular transformations; automatically expanding.\n",
      "left=0.0, right=0.0\n",
      "  'left=%s, right=%s') % (left, right))\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<<<<<<< Topics Terms >>>>>>>\n",
      "\n",
      "    Topic Number                                        Topic Terms\n",
      "0              1  ltlt, goood, eagle, imbored, lips, udeaded, ma...\n",
      "1              2  iwanna, telln, stufff, icatch, matta, tryna, h...\n",
      "2              3  tat, letters, tch, edd, chass, twitter, deynad...\n",
      "3              4  ppl, ladies, fcking, ali, lmaoo, uhuhhh, feeli...\n",
      "4              5  , amber, bettter, liar, gtteam, win, von, youn...\n",
      "5              6  ihit, radios, gunna, circus, gz, bring, bdays,...\n",
      "6              7  igot, standin, love, liee, site, grosss, nicol...\n",
      "7              8  phoney, twitcon, feel, poppin, twin, describe,...\n",
      "8              9  dont, chinese, whiteplains, money, honeys, kil...\n",
      "9             10  mlkshake, apple, people, ee, laughs, woods, vi...\n",
      "10            11  figure, unfollowed, sigh, werk, ugona, uget, g...\n",
      "11            12  baconyup, icnt, gt, ididdd, bert, upset, gonn,...\n",
      "12            13  rag, tha, qot, drunk, sweet, meniggas, accept,...\n",
      "13            14  thinkn, heart, lifeee, bacon, walk, salty, sha...\n",
      "14            15  cute, nigga, hoes, lmaoi, yaaard, lies, number...\n",
      "\n",
      "<<<<<<< Seeded Terms Per Topic >>>>>>>\n",
      "\n",
      "  Topic Number                           Seed Terms:Count\n",
      "0     signal_1                                    bored:1\n",
      "1     signal_9  destroy:1, death:2, kill:1, die:1, hurt:1\n",
      "2     signal_3                              go_to_sleep:1\n",
      "3     signal_6                                 unwanted:1\n"
     ]
    }
   ],
   "source": [
    "detect_depression(pSSLDA_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
